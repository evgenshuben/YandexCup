{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e16aa71b-3d62-414b-9417-79ba98f1cd6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "PROJECT_DIR = '/home/evgenshuben/Desktop/gitReps/YandexCup/'\n",
    "sys.path.append(PROJECT_DIR)\n",
    "os.chdir(PROJECT_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f51c1d1b-448e-4fd9-9393-293d96343de6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from omegaconf import DictConfig, OmegaConf\n",
    "import hydra\n",
    "from hydra import initialize, compose, initialize_config_dir\n",
    "from hydra.utils import instantiate\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "60ab3bcb-c474-4352-8219-0472ae4b888c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Wav2Vec2Processor, AutoModelForCTC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "de5982dc-2366-43e7-85f9-0e150e442f27",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from scr.train import Trainer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c29ed64-cbf7-44dd-ae6a-7d9aaf8b3fbe",
   "metadata": {},
   "source": [
    "# info\n",
    "\n",
    "\n",
    "https://github.com/NVIDIA/NeMo/blob/stable/examples/slu/speech_intent_slot/run_speech_intent_slot_train.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "e2e8bfb9-1be6-414c-bfc8-5c55e5f30594",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NeMo W 2024-10-31 20:17:16 nemo_logging:349] /home/evgenshuben/miniconda3/envs/DL24/lib/python3.10/site-packages/hydra/_internal/defaults_list.py:251: UserWarning: In 'config': Defaults list is missing `_self_`. See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/default_composition_order for more information\n",
      "      warnings.warn(msg, UserWarning)\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "config_dir = '/home/evgenshuben/Desktop/gitReps/YandexCup/configs'\n",
    "config_name = 'config'\n",
    "\n",
    "\n",
    "with initialize_config_dir(config_dir=config_dir, version_base=None):\n",
    "    cfg = compose(config_name=config_name)\n",
    "    # cfg['dataset']['train']['train']['dataset_path']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "ff034dfb-f292-4107-9da0-8201744ac0a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg.enviroment.device = 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "6f8f0f64-7e85-4390-b545-153d2bec4a28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2024-10-31 20:17:22 cloud:58] Found existing object /home/evgenshuben/.cache/torch/NeMo/NeMo_1.23.0/stt_en_conformer_ctc_small/5d2d8e5b2b5adb8f5091363c6ba19c55/stt_en_conformer_ctc_small.nemo.\n",
      "[NeMo I 2024-10-31 20:17:22 cloud:64] Re-using file from: /home/evgenshuben/.cache/torch/NeMo/NeMo_1.23.0/stt_en_conformer_ctc_small/5d2d8e5b2b5adb8f5091363c6ba19c55/stt_en_conformer_ctc_small.nemo\n",
      "[NeMo I 2024-10-31 20:17:22 common:924] Instantiating model from pre-trained checkpoint\n",
      "[NeMo I 2024-10-31 20:17:22 mixins:172] Tokenizer SentencePieceTokenizer initialized with 1024 tokens\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NeMo W 2024-10-31 20:17:23 modelPT:165] If you intend to do training or fine-tuning, please call the ModelPT.setup_training_data() method and provide a valid configuration file to setup the train data loader.\n",
      "    Train config : \n",
      "    manifest_filepath: /data/NeMo_ASR_SET/English/v2.0/train/tarred_audio_manifest.json\n",
      "    sample_rate: 16000\n",
      "    batch_size: 64\n",
      "    shuffle: true\n",
      "    num_workers: 8\n",
      "    pin_memory: true\n",
      "    use_start_end_token: false\n",
      "    trim_silence: false\n",
      "    max_duration: 20.0\n",
      "    min_duration: 0.1\n",
      "    shuffle_n: 2048\n",
      "    is_tarred: true\n",
      "    tarred_audio_filepaths: /data/NeMo_ASR_SET/English/v2.0/train/audio__OP_0..4095_CL_.tar\n",
      "    \n",
      "[NeMo W 2024-10-31 20:17:23 modelPT:172] If you intend to do validation, please call the ModelPT.setup_validation_data() or ModelPT.setup_multiple_validation_data() method and provide a valid configuration file to setup the validation data loader(s). \n",
      "    Validation config : \n",
      "    manifest_filepath:\n",
      "    - /data/ASR/LibriSpeech/librispeech_withsp2/manifests/librivox-dev-other.json\n",
      "    - /data/ASR/LibriSpeech/librispeech_withsp2/manifests/librivox-dev-clean.json\n",
      "    - /data/ASR/LibriSpeech/librispeech_withsp2/manifests/librivox-test-other.json\n",
      "    - /data/ASR/LibriSpeech/librispeech_withsp2/manifests/librivox-test-clean.json\n",
      "    sample_rate: 16000\n",
      "    batch_size: 64\n",
      "    shuffle: false\n",
      "    num_workers: 8\n",
      "    pin_memory: true\n",
      "    use_start_end_token: false\n",
      "    is_tarred: false\n",
      "    tarred_audio_filepaths: na\n",
      "    \n",
      "[NeMo W 2024-10-31 20:17:23 modelPT:178] Please call the ModelPT.setup_test_data() or ModelPT.setup_multiple_test_data() method and provide a valid configuration file to setup the test data loader(s).\n",
      "    Test config : \n",
      "    manifest_filepath:\n",
      "    - /data/ASR/LibriSpeech/librispeech_withsp2/manifests/librivox-test-other.json\n",
      "    - /data/ASR/LibriSpeech/librispeech_withsp2/manifests/librivox-dev-clean.json\n",
      "    - /data/ASR/LibriSpeech/librispeech_withsp2/manifests/librivox-dev-other.json\n",
      "    - /data/ASR/LibriSpeech/librispeech_withsp2/manifests/librivox-test-clean.json\n",
      "    sample_rate: 16000\n",
      "    batch_size: 64\n",
      "    shuffle: false\n",
      "    num_workers: 8\n",
      "    pin_memory: true\n",
      "    use_start_end_token: false\n",
      "    is_tarred: false\n",
      "    tarred_audio_filepaths: na\n",
      "    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2024-10-31 20:17:23 features:289] PADDING: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NeMo W 2024-10-31 20:17:23 nemo_logging:349] /home/evgenshuben/miniconda3/envs/DL24/lib/python3.10/site-packages/nemo/core/connectors/save_restore_connector.py:571: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "      return torch.load(model_weights, map_location='cpu')\n",
      "    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2024-10-31 20:17:23 save_restore_connector:249] Model EncDecCTCModelBPE was successfully restored from /home/evgenshuben/.cache/torch/NeMo/NeMo_1.23.0/stt_en_conformer_ctc_small/5d2d8e5b2b5adb8f5091363c6ba19c55/stt_en_conformer_ctc_small.nemo.\n"
     ]
    }
   ],
   "source": [
    "trainer = Trainer(cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "44ce4cea-695a-4142-8146-ac622b7c457a",
   "metadata": {},
   "outputs": [],
   "source": [
    "for batch in trainer.train_dataloader:\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "7be6f868-6927-4453-917b-bbaac0b21685",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "39535"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.cfg.num_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "85e71571-efb5-4758-aad2-8af624544330",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 84, 50])"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch['anchor'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "b75793e5-61bc-4e0d-9add-5a16515c4638",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "fa3a1046-fcf9-4a7a-b0ce-d3415684cd70",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MixUp:\n",
    "    def __init__(self, num_classes, alpha=1.0):\n",
    "        self.num_classes = num_classes\n",
    "        self.alpha = alpha\n",
    "\n",
    "    def __call__(self, x, y):\n",
    "        if self.alpha > 0:\n",
    "            lam = np.random.beta(self.alpha, self.alpha)\n",
    "        else:\n",
    "            lam = 1\n",
    "\n",
    "        batch_size = x.size(0)\n",
    "        index = torch.randperm(batch_size)\n",
    "\n",
    "        mixed_x = lam * x + (1 - lam) * x[index, :]\n",
    "\n",
    "        # Преобразование y в one-hot векторы\n",
    "        y_one_hot = torch.zeros(batch_size, self.num_classes, device=x.device)\n",
    "        y_one_hot[range(batch_size), y] = 1  # Преобразуем в one-hot\n",
    "\n",
    "        mixed_y = lam * y_one_hot + (1 - lam) * y_one_hot[index]\n",
    "\n",
    "        return mixed_x, mixed_y\n",
    "\n",
    "\n",
    "class CutMix:\n",
    "    def __init__(self, num_classes, alpha=1.0):\n",
    "        self.num_classes = num_classes\n",
    "        self.alpha = alpha\n",
    "\n",
    "    def __call__(self, x, y):\n",
    "        if self.alpha > 0:\n",
    "            lam = np.random.beta(self.alpha, self.alpha)\n",
    "        else:\n",
    "            lam = 1\n",
    "\n",
    "        batch_size = x.size(0)\n",
    "        index = torch.randperm(batch_size)\n",
    "\n",
    "        W = x.size(2)\n",
    "        H = x.size(1)\n",
    "\n",
    "        r_x = np.random.randint(0, W)\n",
    "        r_y = np.random.randint(0, H)\n",
    "        r_w = int(W * np.sqrt(1 - lam))\n",
    "        r_h = int(H * np.sqrt(1 - lam))\n",
    "\n",
    "        r_x1 = np.clip(r_x - r_w // 2, 0, W)\n",
    "        r_x2 = np.clip(r_x + r_w // 2, 0, W)\n",
    "        r_y1 = np.clip(r_y - r_h // 2, 0, H)\n",
    "        r_y2 = np.clip(r_y + r_h // 2, 0, H)\n",
    "\n",
    "        # Создаем смешанные данные\n",
    "        x[:, r_y1:r_y2, r_x1:r_x2] = x[index, r_y1:r_y2, r_x1:r_x2]\n",
    "\n",
    "        # Преобразование y в one-hot векторы\n",
    "        y_one_hot = torch.zeros(batch_size, self.num_classes, device=x.device)\n",
    "        y_one_hot[range(batch_size), y] = 1  # Преобразуем в one-hot\n",
    "\n",
    "        # Смешиваем метки\n",
    "        y_a = y_one_hot\n",
    "        y_b = y_one_hot[index]\n",
    "        mixed_y = lam * y_a + (1 - lam) * y_b\n",
    "\n",
    "        return x, mixed_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "0d8a79a1-3c14-498b-87d4-821ebcf80419",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AugmentationCollator:\n",
    "    def __init__(self, augmentations):\n",
    "        self.augmentations = augmentations\n",
    "\n",
    "    def __call__(self, batch):\n",
    "\n",
    "        anchor_ids = [item['anchor_id'] for item in batch]  # ID без изменений\n",
    "        anchors = torch.stack([item['anchor'] for item in batch])  # Тензор изображений\n",
    "        labels = torch.stack([item['anchor_label'] for item in batch])  # Тензор меток\n",
    "\n",
    "        augmented_anchors, augmented_labels = self.augmentations(anchors, labels.long())\n",
    "\n",
    "        return {\n",
    "            'anchor_id': anchor_ids,\n",
    "            'anchor': augmented_anchors,\n",
    "            'anchor_label': augmented_labels\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "44817817-5f60-48c3-aeb2-9f2e19f84be9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.transforms.v2 import RandomChoice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "b0698be7-5281-4286-bc44-9437f595f650",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "dc2c6987-da5f-4566-b175-e3d40fe83a75",
   "metadata": {},
   "outputs": [],
   "source": [
    "augmentations = RandomChoice([\n",
    "            MixUp(num_classes=num_classes),\n",
    "            CutMix(num_classes=num_classes)\n",
    "        ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "241a2ebd-22ce-4174-a206-fe6f37bd3815",
   "metadata": {},
   "outputs": [],
   "source": [
    "collator = AugmentationCollator(augmentations=augmentations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "6bcd35ea-2354-4a22-a0a9-9abc095ce90c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([84, 50])"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train_dataloader.dataset[0]['anchor'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "1a018f22-8833-4e0c-9c25-0b72fae286cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_loader = DataLoader(trainer.train_dataloader.dataset, batch_size=32, collate_fn=collator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "59feb6ac-e71c-42f1-be41-c43cefdc0dd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "for batch in data_loader:\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ecd340f-a981-4430-abda-942ba2b25369",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "daf31a5b-f138-47f4-a698-2969d7cf868d",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader = trainer.train_dataloader.dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "4cb1eb6c-9e85-4e4a-a067-e157304fae77",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'DataLoader' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[96], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mDataLoader\u001b[49m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'DataLoader' is not defined"
     ]
    }
   ],
   "source": [
    "DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "f612d8eb-3bbd-4c78-b35a-28418e82e269",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = trainer.cfg.num_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "f439e79e-08af-49dc-adc2-1d1b7c267f27",
   "metadata": {},
   "outputs": [],
   "source": [
    "anchors = batch['anchor']\n",
    "labels = batch['anchor_label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "65dc07fd-3787-494c-93ae-0e23371a7c07",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "04824388-b505-45c1-b687-08422f8a17ab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-27.4922, -24.0988, -18.5980,  ..., -25.0063, -23.5181, -22.5378],\n",
       "         [-17.8997, -19.3798, -20.3946,  ..., -29.3251, -34.3992, -29.5529],\n",
       "         [-20.1813, -19.8249, -19.6386,  ..., -27.8654, -26.7650, -25.3741],\n",
       "         ...,\n",
       "         [-28.4069, -27.6728, -35.5562,  ..., -31.4960, -28.2954, -27.0652],\n",
       "         [-25.7478, -26.3863, -27.0308,  ..., -20.7767, -22.2248, -24.9296],\n",
       "         [-30.0559, -24.8653, -30.4569,  ..., -25.3193, -24.2758, -25.0753]],\n",
       "\n",
       "        [[-49.4972, -47.4558, -48.4513,  ..., -37.9406, -29.7031, -25.1541],\n",
       "         [-47.4412, -51.0624, -47.0224,  ..., -38.8281, -34.4013, -29.2891],\n",
       "         [-39.1572, -38.4866, -42.7885,  ..., -31.9471, -32.9739, -29.4244],\n",
       "         ...,\n",
       "         [-35.7585, -37.4499, -25.9650,  ..., -17.9286, -15.8398, -14.8191],\n",
       "         [-42.6736, -39.8472, -33.4855,  ..., -20.8357, -21.0604, -27.2990],\n",
       "         [-39.7250, -42.1776, -34.2852,  ..., -24.2008, -23.2768, -28.7641]],\n",
       "\n",
       "        [[-27.8135, -28.4518, -31.7045,  ..., -23.8242, -21.5151, -23.2354],\n",
       "         [-26.7594, -25.2457, -27.7852,  ..., -21.6258, -19.7977, -16.1690],\n",
       "         [-26.1312, -27.1684, -28.9467,  ..., -22.7413, -20.6736, -21.9982],\n",
       "         ...,\n",
       "         [-31.9297, -33.1868, -33.0911,  ..., -33.4877, -33.7129, -34.9169],\n",
       "         [-32.0540, -32.3060, -34.4886,  ..., -34.1604, -35.8254, -34.2645],\n",
       "         [-32.7121, -34.8832, -34.3809,  ..., -34.7780, -35.4220, -35.3546]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[-49.1532, -40.8803, -39.2161,  ..., -42.7662, -37.4807, -31.5131],\n",
       "         [-40.2394, -36.7962, -38.6625,  ..., -39.1512, -41.7777, -35.3162],\n",
       "         [-45.5259, -40.1550, -38.3142,  ..., -30.9532, -34.0424, -33.6293],\n",
       "         ...,\n",
       "         [-26.0236, -35.6610, -38.9663,  ..., -23.4381, -26.1421, -21.9943],\n",
       "         [-23.6450, -36.1238, -38.3396,  ..., -35.7716, -37.9209, -30.4351],\n",
       "         [-25.5279, -31.0270, -36.1265,  ..., -34.3405, -29.8218, -23.1068]],\n",
       "\n",
       "        [[-43.2397, -47.5690, -44.8321,  ..., -37.1320, -40.8039, -40.4756],\n",
       "         [-47.8893, -47.4457, -50.5862,  ..., -37.2134, -38.2947, -36.7734],\n",
       "         [-41.3061, -41.9914, -42.6590,  ..., -47.1765, -52.9358, -47.0662],\n",
       "         ...,\n",
       "         [-37.5756, -30.7740, -30.7918,  ..., -46.1784, -45.8605, -46.9332],\n",
       "         [-37.3425, -33.6240, -33.7371,  ..., -42.0653, -37.2347, -33.4429],\n",
       "         [-32.6206, -29.4959, -32.4528,  ..., -41.0944, -45.4393, -47.4142]],\n",
       "\n",
       "        [[-23.0897, -25.9261, -28.0970,  ..., -18.4523, -17.0507, -23.1211],\n",
       "         [-27.2884, -22.0247, -17.0878,  ..., -22.3793, -21.4583, -18.3857],\n",
       "         [-23.6886, -24.7570, -18.9475,  ..., -19.7785, -20.0404, -15.6762],\n",
       "         ...,\n",
       "         [-26.9306, -25.6207, -20.1059,  ..., -26.9576, -19.7823, -16.0556],\n",
       "         [-21.7874, -19.4358, -10.7836,  ..., -20.0816, -27.3080, -24.4327],\n",
       "         [-28.0475, -28.9504, -19.5764,  ..., -27.9963, -24.4709, -20.7409]]])"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "augmentations[0](anchors, labels.long())[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca2206e0-45d4-402e-b777-fa1abd81a7a1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4722e8a-2ec1-45be-b40f-55e4a37d0a82",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c8ce118-f7d7-438e-b674-aee9509b964c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ed4e2ad-0ff9-40a1-884f-c6d17d0a7077",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "id": "e68b8e2a-30b9-4a5e-9f53-029e553d8e67",
   "metadata": {},
   "outputs": [],
   "source": [
    "specrtogramm = trainer.train_dataloader.dataset[0]['anchor']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "id": "369bcd54-48a2-47ba-8f09-6758f1be08de",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([80, 50])"
      ]
     },
     "execution_count": 201,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "specrtogramm.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "id": "2e62fa04-9a59-4b7f-9590-76485590209c",
   "metadata": {},
   "outputs": [],
   "source": [
    "for batch in trainer.train_dataloader:\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "id": "03947d8a-e0ce-461e-8d8d-ca616b0c0dec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 80, 50])"
      ]
     },
     "execution_count": 203,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch['anchor'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9399c6b3-9dd6-40eb-933a-9400bf240cfe",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "e4b15e48-aa2d-4230-8503-e2dc58318109",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = asr_model.encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "9c868650-b791-4af4-bcdd-f02d0894661a",
   "metadata": {},
   "outputs": [],
   "source": [
    "mel_spectrograms = batch['anchor']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "a428fca4-6a92-48f9-b6d1-deb650a83eae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 80, 50])"
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mel_spectrograms.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "id": "b989ed2d-61c4-4a38-84b4-47a339f3788d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nemo.collections.asr.models import ASRModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "id": "afd95531-16e4-4264-b967-2019dbe18449",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'stt_en_conformer_ctc_small'"
      ]
     },
     "execution_count": 211,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pretrained_encoder_name"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18c554dd-3cf1-4bf6-9246-c6edb2b0cd04",
   "metadata": {},
   "source": [
    "# линейный слой для интерполяции"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "fefb4974-3633-40e6-a52a-643e3f8d1a43",
   "metadata": {},
   "outputs": [],
   "source": [
    "for batch in trainer.train_dataloader:\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "693de9cf-b777-47c2-bcbc-50e46492337c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 84, 50])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch['anchor'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "203f691b-39dc-40e7-8a51-db5d206e55c7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "b97174c2-f5ac-4dc3-a4c7-25b76bee69b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = batch['anchor']\n",
    "\n",
    "batch_size, freq_size, time_size = x.size()\n",
    "x = x.permute(0, 2, 1).reshape(-1, freq_size)  # Приводим к (batch_size * time_size, 84)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "b553c32a-04a3-4309-bc58-ffc8ed5d3dbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "freq_transform = nn.Linear(84, 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "c658d29f-fbd1-4577-876a-886b0f722c1c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3200, 80])"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = freq_transform(x)\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "feaaa3f2-f0e2-45f7-ba4b-31d93f96853e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "64"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "0663ca0f-8105-4798-982a-f593896bdfd8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 50, 80])"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.view(batch_size, time_size, -1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cc5cf5f-5ef5-456d-ab1d-054accb1e83f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1645eee7-b17e-466a-8b26-30a9096afaad",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f104424-6d94-4a91-9db9-5ed6b6acf1ff",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "e27bb711-1d6a-4b88-a7b3-6e09934824b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = x.view(batch_size, time_size, -1).permute(0, 2, 1)  # Теперь (batch_size, 80, 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "5e17974a-3e83-4afe-af00-131b85a169c8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 84, 50])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e28a7ba8-9962-48d2-ac48-ba4062054844",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "id": "ce7ff39c-de7e-4b3b-bb53-7a3ee5cf5846",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AudioConformer(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        pretrained_model_name='stt_en_conformer_ctc_small',\n",
    "        emb_size=512,\n",
    "        num_classes=10\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.pretrained_model = ASRModel.from_pretrained(\n",
    "            model_name=pretrained_model_name,\n",
    "            # map_location=torch.device(\"cuda\")\n",
    "        )\n",
    "\n",
    "        self.encoder = self.pretrained_model.encoder\n",
    "        \n",
    "        self.embedding_size = self.encoder.pre_encode.out.out_features\n",
    "        \n",
    "        self.fc_emb = nn.Linear(self.embedding_size, emb_size) if emb_size != self.embedding_size else nn.Identity()\n",
    "\n",
    "        self.clf = nn.Linear(emb_size, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        encoder_output, lengths = self.encoder(\n",
    "            audio_signal=x,\n",
    "            length=torch.tensor([x.shape[2]] * x.shape[0], device=x.device)  # Убедитесь, что длины находятся на том же устройстве\n",
    "        )\n",
    "        \n",
    "        features = encoder_output.mean(dim=2)  # Усредняем по временной оси\n",
    "        emb = self.fc_emb(features)\n",
    "\n",
    "        cls = self.clf(emb)\n",
    "        return dict(emb=emb, cls=cls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "id": "52651389-4108-43fd-bdef-e4346b830519",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = batch['anchor']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "id": "43176762-5bf5-4ec5-9745-96e02ae933d1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50,\n",
       "        50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50,\n",
       "        50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50,\n",
       "        50, 50, 50, 50, 50, 50, 50, 50, 50, 50])"
      ]
     },
     "execution_count": 285,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.tensor([x.shape[2]] * x.shape[0], device=x.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "471a6096-42b9-4b68-ac97-a2eb4c4f5505",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "id": "3d15198d-b07e-42da-93df-1f88f34fbd80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2024-10-31 19:11:40 cloud:58] Found existing object /home/evgenshuben/.cache/torch/NeMo/NeMo_1.23.0/stt_en_conformer_ctc_small/5d2d8e5b2b5adb8f5091363c6ba19c55/stt_en_conformer_ctc_small.nemo.\n",
      "[NeMo I 2024-10-31 19:11:40 cloud:64] Re-using file from: /home/evgenshuben/.cache/torch/NeMo/NeMo_1.23.0/stt_en_conformer_ctc_small/5d2d8e5b2b5adb8f5091363c6ba19c55/stt_en_conformer_ctc_small.nemo\n",
      "[NeMo I 2024-10-31 19:11:40 common:924] Instantiating model from pre-trained checkpoint\n",
      "[NeMo I 2024-10-31 19:11:41 mixins:172] Tokenizer SentencePieceTokenizer initialized with 1024 tokens\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NeMo W 2024-10-31 19:11:41 modelPT:165] If you intend to do training or fine-tuning, please call the ModelPT.setup_training_data() method and provide a valid configuration file to setup the train data loader.\n",
      "    Train config : \n",
      "    manifest_filepath: /data/NeMo_ASR_SET/English/v2.0/train/tarred_audio_manifest.json\n",
      "    sample_rate: 16000\n",
      "    batch_size: 64\n",
      "    shuffle: true\n",
      "    num_workers: 8\n",
      "    pin_memory: true\n",
      "    use_start_end_token: false\n",
      "    trim_silence: false\n",
      "    max_duration: 20.0\n",
      "    min_duration: 0.1\n",
      "    shuffle_n: 2048\n",
      "    is_tarred: true\n",
      "    tarred_audio_filepaths: /data/NeMo_ASR_SET/English/v2.0/train/audio__OP_0..4095_CL_.tar\n",
      "    \n",
      "[NeMo W 2024-10-31 19:11:41 modelPT:172] If you intend to do validation, please call the ModelPT.setup_validation_data() or ModelPT.setup_multiple_validation_data() method and provide a valid configuration file to setup the validation data loader(s). \n",
      "    Validation config : \n",
      "    manifest_filepath:\n",
      "    - /data/ASR/LibriSpeech/librispeech_withsp2/manifests/librivox-dev-other.json\n",
      "    - /data/ASR/LibriSpeech/librispeech_withsp2/manifests/librivox-dev-clean.json\n",
      "    - /data/ASR/LibriSpeech/librispeech_withsp2/manifests/librivox-test-other.json\n",
      "    - /data/ASR/LibriSpeech/librispeech_withsp2/manifests/librivox-test-clean.json\n",
      "    sample_rate: 16000\n",
      "    batch_size: 64\n",
      "    shuffle: false\n",
      "    num_workers: 8\n",
      "    pin_memory: true\n",
      "    use_start_end_token: false\n",
      "    is_tarred: false\n",
      "    tarred_audio_filepaths: na\n",
      "    \n",
      "[NeMo W 2024-10-31 19:11:41 modelPT:178] Please call the ModelPT.setup_test_data() or ModelPT.setup_multiple_test_data() method and provide a valid configuration file to setup the test data loader(s).\n",
      "    Test config : \n",
      "    manifest_filepath:\n",
      "    - /data/ASR/LibriSpeech/librispeech_withsp2/manifests/librivox-test-other.json\n",
      "    - /data/ASR/LibriSpeech/librispeech_withsp2/manifests/librivox-dev-clean.json\n",
      "    - /data/ASR/LibriSpeech/librispeech_withsp2/manifests/librivox-dev-other.json\n",
      "    - /data/ASR/LibriSpeech/librispeech_withsp2/manifests/librivox-test-clean.json\n",
      "    sample_rate: 16000\n",
      "    batch_size: 64\n",
      "    shuffle: false\n",
      "    num_workers: 8\n",
      "    pin_memory: true\n",
      "    use_start_end_token: false\n",
      "    is_tarred: false\n",
      "    tarred_audio_filepaths: na\n",
      "    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2024-10-31 19:11:41 features:289] PADDING: 0\n",
      "[NeMo I 2024-10-31 19:11:41 save_restore_connector:249] Model EncDecCTCModelBPE was successfully restored from /home/evgenshuben/.cache/torch/NeMo/NeMo_1.23.0/stt_en_conformer_ctc_small/5d2d8e5b2b5adb8f5091363c6ba19c55/stt_en_conformer_ctc_small.nemo.\n"
     ]
    }
   ],
   "source": [
    "model = AudioConformer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "id": "872c1cf5-b632-442f-9900-8a58d2a4e80b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AudioConformer(\n",
       "  (pretrained_model): EncDecCTCModelBPE(\n",
       "    (preprocessor): AudioToMelSpectrogramPreprocessor(\n",
       "      (featurizer): FilterbankFeatures()\n",
       "    )\n",
       "    (encoder): ConformerEncoder(\n",
       "      (pre_encode): ConvSubsampling(\n",
       "        (out): Linear(in_features=3520, out_features=176, bias=True)\n",
       "        (conv): Sequential(\n",
       "          (0): Conv2d(1, 176, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "          (1): ReLU(inplace=True)\n",
       "          (2): Conv2d(176, 176, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "          (3): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "      (pos_enc): RelPositionalEncoding(\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (layers): ModuleList(\n",
       "        (0-15): 16 x ConformerLayer(\n",
       "          (norm_feed_forward1): LayerNorm((176,), eps=1e-05, elementwise_affine=True)\n",
       "          (feed_forward1): ConformerFeedForward(\n",
       "            (linear1): Linear(in_features=176, out_features=704, bias=True)\n",
       "            (activation): Swish()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (linear2): Linear(in_features=704, out_features=176, bias=True)\n",
       "          )\n",
       "          (norm_conv): LayerNorm((176,), eps=1e-05, elementwise_affine=True)\n",
       "          (conv): ConformerConvolution(\n",
       "            (pointwise_conv1): Conv1d(176, 352, kernel_size=(1,), stride=(1,))\n",
       "            (depthwise_conv): CausalConv1D(176, 176, kernel_size=(31,), stride=(1,), groups=176)\n",
       "            (batch_norm): BatchNorm1d(176, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (activation): Swish()\n",
       "            (pointwise_conv2): Conv1d(176, 176, kernel_size=(1,), stride=(1,))\n",
       "          )\n",
       "          (norm_self_att): LayerNorm((176,), eps=1e-05, elementwise_affine=True)\n",
       "          (self_attn): RelPositionMultiHeadAttention(\n",
       "            (linear_q): Linear(in_features=176, out_features=176, bias=True)\n",
       "            (linear_k): Linear(in_features=176, out_features=176, bias=True)\n",
       "            (linear_v): Linear(in_features=176, out_features=176, bias=True)\n",
       "            (linear_out): Linear(in_features=176, out_features=176, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (linear_pos): Linear(in_features=176, out_features=176, bias=False)\n",
       "          )\n",
       "          (norm_feed_forward2): LayerNorm((176,), eps=1e-05, elementwise_affine=True)\n",
       "          (feed_forward2): ConformerFeedForward(\n",
       "            (linear1): Linear(in_features=176, out_features=704, bias=True)\n",
       "            (activation): Swish()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (linear2): Linear(in_features=704, out_features=176, bias=True)\n",
       "          )\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (norm_out): LayerNorm((176,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (decoder): ConvASRDecoder(\n",
       "      (decoder_layers): Sequential(\n",
       "        (0): Conv1d(176, 1025, kernel_size=(1,), stride=(1,))\n",
       "      )\n",
       "    )\n",
       "    (loss): CTCLoss()\n",
       "    (spec_augmentation): SpectrogramAugmentation(\n",
       "      (spec_augment): SpecAugment()\n",
       "    )\n",
       "    (wer): WER()\n",
       "  )\n",
       "  (encoder): ConformerEncoder(\n",
       "    (pre_encode): ConvSubsampling(\n",
       "      (out): Linear(in_features=3520, out_features=176, bias=True)\n",
       "      (conv): Sequential(\n",
       "        (0): Conv2d(1, 176, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "        (1): ReLU(inplace=True)\n",
       "        (2): Conv2d(176, 176, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "        (3): ReLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (pos_enc): RelPositionalEncoding(\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (layers): ModuleList(\n",
       "      (0-15): 16 x ConformerLayer(\n",
       "        (norm_feed_forward1): LayerNorm((176,), eps=1e-05, elementwise_affine=True)\n",
       "        (feed_forward1): ConformerFeedForward(\n",
       "          (linear1): Linear(in_features=176, out_features=704, bias=True)\n",
       "          (activation): Swish()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (linear2): Linear(in_features=704, out_features=176, bias=True)\n",
       "        )\n",
       "        (norm_conv): LayerNorm((176,), eps=1e-05, elementwise_affine=True)\n",
       "        (conv): ConformerConvolution(\n",
       "          (pointwise_conv1): Conv1d(176, 352, kernel_size=(1,), stride=(1,))\n",
       "          (depthwise_conv): CausalConv1D(176, 176, kernel_size=(31,), stride=(1,), groups=176)\n",
       "          (batch_norm): BatchNorm1d(176, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (activation): Swish()\n",
       "          (pointwise_conv2): Conv1d(176, 176, kernel_size=(1,), stride=(1,))\n",
       "        )\n",
       "        (norm_self_att): LayerNorm((176,), eps=1e-05, elementwise_affine=True)\n",
       "        (self_attn): RelPositionMultiHeadAttention(\n",
       "          (linear_q): Linear(in_features=176, out_features=176, bias=True)\n",
       "          (linear_k): Linear(in_features=176, out_features=176, bias=True)\n",
       "          (linear_v): Linear(in_features=176, out_features=176, bias=True)\n",
       "          (linear_out): Linear(in_features=176, out_features=176, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (linear_pos): Linear(in_features=176, out_features=176, bias=False)\n",
       "        )\n",
       "        (norm_feed_forward2): LayerNorm((176,), eps=1e-05, elementwise_affine=True)\n",
       "        (feed_forward2): ConformerFeedForward(\n",
       "          (linear1): Linear(in_features=176, out_features=704, bias=True)\n",
       "          (activation): Swish()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (linear2): Linear(in_features=704, out_features=176, bias=True)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (norm_out): LayerNorm((176,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (fc_emb): Linear(in_features=176, out_features=512, bias=True)\n",
       "  (clf): Linear(in_features=512, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 287,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.to('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "id": "37e38303-51cb-4958-bc3a-6bb8dfa577ad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 512])"
      ]
     },
     "execution_count": 288,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(x.to('cuda'))['emb'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "id": "c5fce920-ee23-4e91-921a-9fc57cbdf5ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_params = sum(p.numel() for p in model.parameters() if p.requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "id": "d9a86797-e647-4e73-932f-3a13bcd4ed14",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13249787"
      ]
     },
     "execution_count": 296,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "125a8c43-e6b7-4741-ad62-86debae70959",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "aa2b84e9-d97c-49bf-bf11-885f55bde2f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from nemo.collections.asr.models import ASRModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "05d634c2-c1f3-45bc-bacb-fffc5cd39fad",
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrained_encoder_name = 'stt_en_conformer_ctc_small'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3ba1ea65-194b-4606-91c9-08c28c1dab75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2024-10-31 19:52:46 cloud:58] Found existing object /home/evgenshuben/.cache/torch/NeMo/NeMo_1.23.0/stt_en_conformer_ctc_small/5d2d8e5b2b5adb8f5091363c6ba19c55/stt_en_conformer_ctc_small.nemo.\n",
      "[NeMo I 2024-10-31 19:52:46 cloud:64] Re-using file from: /home/evgenshuben/.cache/torch/NeMo/NeMo_1.23.0/stt_en_conformer_ctc_small/5d2d8e5b2b5adb8f5091363c6ba19c55/stt_en_conformer_ctc_small.nemo\n",
      "[NeMo I 2024-10-31 19:52:46 common:924] Instantiating model from pre-trained checkpoint\n",
      "[NeMo I 2024-10-31 19:52:47 mixins:172] Tokenizer SentencePieceTokenizer initialized with 1024 tokens\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NeMo W 2024-10-31 19:52:47 modelPT:165] If you intend to do training or fine-tuning, please call the ModelPT.setup_training_data() method and provide a valid configuration file to setup the train data loader.\n",
      "    Train config : \n",
      "    manifest_filepath: /data/NeMo_ASR_SET/English/v2.0/train/tarred_audio_manifest.json\n",
      "    sample_rate: 16000\n",
      "    batch_size: 64\n",
      "    shuffle: true\n",
      "    num_workers: 8\n",
      "    pin_memory: true\n",
      "    use_start_end_token: false\n",
      "    trim_silence: false\n",
      "    max_duration: 20.0\n",
      "    min_duration: 0.1\n",
      "    shuffle_n: 2048\n",
      "    is_tarred: true\n",
      "    tarred_audio_filepaths: /data/NeMo_ASR_SET/English/v2.0/train/audio__OP_0..4095_CL_.tar\n",
      "    \n",
      "[NeMo W 2024-10-31 19:52:47 modelPT:172] If you intend to do validation, please call the ModelPT.setup_validation_data() or ModelPT.setup_multiple_validation_data() method and provide a valid configuration file to setup the validation data loader(s). \n",
      "    Validation config : \n",
      "    manifest_filepath:\n",
      "    - /data/ASR/LibriSpeech/librispeech_withsp2/manifests/librivox-dev-other.json\n",
      "    - /data/ASR/LibriSpeech/librispeech_withsp2/manifests/librivox-dev-clean.json\n",
      "    - /data/ASR/LibriSpeech/librispeech_withsp2/manifests/librivox-test-other.json\n",
      "    - /data/ASR/LibriSpeech/librispeech_withsp2/manifests/librivox-test-clean.json\n",
      "    sample_rate: 16000\n",
      "    batch_size: 64\n",
      "    shuffle: false\n",
      "    num_workers: 8\n",
      "    pin_memory: true\n",
      "    use_start_end_token: false\n",
      "    is_tarred: false\n",
      "    tarred_audio_filepaths: na\n",
      "    \n",
      "[NeMo W 2024-10-31 19:52:47 modelPT:178] Please call the ModelPT.setup_test_data() or ModelPT.setup_multiple_test_data() method and provide a valid configuration file to setup the test data loader(s).\n",
      "    Test config : \n",
      "    manifest_filepath:\n",
      "    - /data/ASR/LibriSpeech/librispeech_withsp2/manifests/librivox-test-other.json\n",
      "    - /data/ASR/LibriSpeech/librispeech_withsp2/manifests/librivox-dev-clean.json\n",
      "    - /data/ASR/LibriSpeech/librispeech_withsp2/manifests/librivox-dev-other.json\n",
      "    - /data/ASR/LibriSpeech/librispeech_withsp2/manifests/librivox-test-clean.json\n",
      "    sample_rate: 16000\n",
      "    batch_size: 64\n",
      "    shuffle: false\n",
      "    num_workers: 8\n",
      "    pin_memory: true\n",
      "    use_start_end_token: false\n",
      "    is_tarred: false\n",
      "    tarred_audio_filepaths: na\n",
      "    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2024-10-31 19:52:47 features:289] PADDING: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NeMo W 2024-10-31 19:52:47 nemo_logging:349] /home/evgenshuben/miniconda3/envs/DL24/lib/python3.10/site-packages/nemo/core/connectors/save_restore_connector.py:571: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "      return torch.load(model_weights, map_location='cpu')\n",
      "    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2024-10-31 19:52:47 save_restore_connector:249] Model EncDecCTCModelBPE was successfully restored from /home/evgenshuben/.cache/torch/NeMo/NeMo_1.23.0/stt_en_conformer_ctc_small/5d2d8e5b2b5adb8f5091363c6ba19c55/stt_en_conformer_ctc_small.nemo.\n"
     ]
    }
   ],
   "source": [
    "model_cls = ASRModel\n",
    "\n",
    "pretraind_model = model_cls.from_pretrained(\n",
    "    model_name=pretrained_encoder_name, map_location=torch.device(\"cpu\")\n",
    ")\n",
    "\n",
    "encoder = pretraind_model.encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "62d74619-6cfa-4fd9-a790-e769cd2649e1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ConformerEncoder(\n",
       "  (pre_encode): ConvSubsampling(\n",
       "    (out): Linear(in_features=3520, out_features=176, bias=True)\n",
       "    (conv): Sequential(\n",
       "      (0): Conv2d(1, 176, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "      (1): ReLU(inplace=True)\n",
       "      (2): Conv2d(176, 176, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "      (3): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (pos_enc): RelPositionalEncoding(\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (layers): ModuleList(\n",
       "    (0-15): 16 x ConformerLayer(\n",
       "      (norm_feed_forward1): LayerNorm((176,), eps=1e-05, elementwise_affine=True)\n",
       "      (feed_forward1): ConformerFeedForward(\n",
       "        (linear1): Linear(in_features=176, out_features=704, bias=True)\n",
       "        (activation): Swish()\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (linear2): Linear(in_features=704, out_features=176, bias=True)\n",
       "      )\n",
       "      (norm_conv): LayerNorm((176,), eps=1e-05, elementwise_affine=True)\n",
       "      (conv): ConformerConvolution(\n",
       "        (pointwise_conv1): Conv1d(176, 352, kernel_size=(1,), stride=(1,))\n",
       "        (depthwise_conv): CausalConv1D(176, 176, kernel_size=(31,), stride=(1,), groups=176)\n",
       "        (batch_norm): BatchNorm1d(176, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (activation): Swish()\n",
       "        (pointwise_conv2): Conv1d(176, 176, kernel_size=(1,), stride=(1,))\n",
       "      )\n",
       "      (norm_self_att): LayerNorm((176,), eps=1e-05, elementwise_affine=True)\n",
       "      (self_attn): RelPositionMultiHeadAttention(\n",
       "        (linear_q): Linear(in_features=176, out_features=176, bias=True)\n",
       "        (linear_k): Linear(in_features=176, out_features=176, bias=True)\n",
       "        (linear_v): Linear(in_features=176, out_features=176, bias=True)\n",
       "        (linear_out): Linear(in_features=176, out_features=176, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (linear_pos): Linear(in_features=176, out_features=176, bias=False)\n",
       "      )\n",
       "      (norm_feed_forward2): LayerNorm((176,), eps=1e-05, elementwise_affine=True)\n",
       "      (feed_forward2): ConformerFeedForward(\n",
       "        (linear1): Linear(in_features=176, out_features=704, bias=True)\n",
       "        (activation): Swish()\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (linear2): Linear(in_features=704, out_features=176, bias=True)\n",
       "      )\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "      (norm_out): LayerNorm((176,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ec7b638-5be6-4dc2-af2d-6265fd64fd09",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abd82986-28a0-4bad-aac6-92fc4a955fd6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c870fa3-3650-4cbd-9fe2-7606ff119216",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "id": "a13d5f6f-a79b-4a3f-bed8-e474e8a6419f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32"
      ]
     },
     "execution_count": 212,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mel_spectrograms.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "id": "7e91733d-0595-472d-9db8-60f160a12cf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_output = encoder(audio_signal=mel_spectrograms, length=torch.tensor([mel_spectrograms.shape[2]] * mel_spectrograms.shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "id": "df5af060-2238-4376-89bb-282f2b5671b1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 176, 13])"
      ]
     },
     "execution_count": 216,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder_output[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "id": "3dd68cbf-ae27-489b-a029-0792129c0ee6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32])"
      ]
     },
     "execution_count": 217,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder_output[1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0407b98d-f415-4e47-b51a-fad00e406009",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd881949-362a-4f91-a18e-c0061ab37b6d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1fcc0c3-96b4-4c7e-bb37-3d1c03d30d9f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2219ec53-e5af-45a7-8f55-71fa130f0364",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cea983c9-9169-44f9-9dce-b7a130d8e61d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a14cf1f6-beaa-42b3-aca7-b7e94a3813fa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "9dbe2356-4a7f-4d38-81b5-755d808b5de8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# encoder = asr_model.encoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "b11d385c-132e-4ddf-87cf-413fc317791e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# asr_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "63603414-a80b-4028-834e-2890f8dbf8f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# encoder = asr_model.encoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "c5a573a0-e03f-406a-98e2-8e80ba945340",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nemo.collections.asr.modules import ConformerEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "7b4a4f6a-5430-4536-a592-898ecde7dfc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# encoder = asr_model.encoder(\n",
    "#     feat_in=80,\n",
    "#     n_layers=18,\n",
    "#     d_model=512,\n",
    "#     feat_out=-1\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "8d402d9a-cf13-40bc-a389-0d0552c797d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = asr_model.encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "b4e69e3e-3581-477f-9240-5da50728f116",
   "metadata": {},
   "outputs": [],
   "source": [
    "mel_spectrograms = batch['anchor']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "df234f13-0dd8-4e86-9ac3-c261c86822b5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 80, 50])"
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mel_spectrograms.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "ed917c28-492c-4db3-9a8c-08122917b45b",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Input type (torch.FloatTensor) and weight type (torch.cuda.FloatTensor) should be the same or input should be a MKLDNN tensor and weight is a dense tensor",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[170], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m encoder_output \u001b[38;5;241m=\u001b[39m \u001b[43mencoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43maudio_signal\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmel_spectrograms\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlength\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mmel_spectrograms\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m64\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/DL24/lib/python3.10/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/DL24/lib/python3.10/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/miniconda3/envs/DL24/lib/python3.10/site-packages/nemo/core/classes/common.py:1098\u001b[0m, in \u001b[0;36mtypecheck.__call__\u001b[0;34m(self, wrapped, instance, args, kwargs)\u001b[0m\n\u001b[1;32m   1095\u001b[0m instance\u001b[38;5;241m.\u001b[39m_validate_input_types(input_types\u001b[38;5;241m=\u001b[39minput_types, ignore_collections\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mignore_collections, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1097\u001b[0m \u001b[38;5;66;03m# Call the method - this can be forward, or any other callable method\u001b[39;00m\n\u001b[0;32m-> 1098\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mwrapped\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1100\u001b[0m instance\u001b[38;5;241m.\u001b[39m_attach_and_validate_output_types(\n\u001b[1;32m   1101\u001b[0m     output_types\u001b[38;5;241m=\u001b[39moutput_types, ignore_collections\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mignore_collections, out_objects\u001b[38;5;241m=\u001b[39moutputs\n\u001b[1;32m   1102\u001b[0m )\n\u001b[1;32m   1104\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m outputs\n",
      "File \u001b[0;32m~/miniconda3/envs/DL24/lib/python3.10/site-packages/nemo/collections/asr/modules/conformer_encoder.py:491\u001b[0m, in \u001b[0;36mConformerEncoder.forward\u001b[0;34m(self, audio_signal, length, cache_last_channel, cache_last_time, cache_last_channel_len)\u001b[0m\n\u001b[1;32m    487\u001b[0m \u001b[38;5;129m@typecheck\u001b[39m()\n\u001b[1;32m    488\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\n\u001b[1;32m    489\u001b[0m     \u001b[38;5;28mself\u001b[39m, audio_signal, length, cache_last_channel\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, cache_last_time\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, cache_last_channel_len\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    490\u001b[0m ):\n\u001b[0;32m--> 491\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward_internal\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    492\u001b[0m \u001b[43m        \u001b[49m\u001b[43maudio_signal\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    493\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlength\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    494\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcache_last_channel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_last_channel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    495\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcache_last_time\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_last_time\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    496\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcache_last_channel_len\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_last_channel_len\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    497\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/DL24/lib/python3.10/site-packages/nemo/collections/asr/modules/conformer_encoder.py:521\u001b[0m, in \u001b[0;36mConformerEncoder.forward_internal\u001b[0;34m(self, audio_signal, length, cache_last_channel, cache_last_time, cache_last_channel_len)\u001b[0m\n\u001b[1;32m    519\u001b[0m     audio_signal \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpre_encode(audio_signal)\n\u001b[1;32m    520\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 521\u001b[0m     audio_signal, length \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpre_encode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maudio_signal\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlengths\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlength\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    522\u001b[0m     length \u001b[38;5;241m=\u001b[39m length\u001b[38;5;241m.\u001b[39mto(torch\u001b[38;5;241m.\u001b[39mint64)\n\u001b[1;32m    523\u001b[0m     \u001b[38;5;66;03m# self.streaming_cfg is set by setup_streaming_cfg(), called in the init\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/DL24/lib/python3.10/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/DL24/lib/python3.10/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/miniconda3/envs/DL24/lib/python3.10/site-packages/nemo/collections/asr/parts/submodules/subsampling.py:425\u001b[0m, in \u001b[0;36mConvSubsampling.forward\u001b[0;34m(self, x, lengths)\u001b[0m\n\u001b[1;32m    423\u001b[0m                 x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconv(x)  \u001b[38;5;66;03m# try anyway\u001b[39;00m\n\u001b[1;32m    424\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 425\u001b[0m         x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    426\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    427\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconv(x)\n",
      "File \u001b[0;32m~/miniconda3/envs/DL24/lib/python3.10/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/DL24/lib/python3.10/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/miniconda3/envs/DL24/lib/python3.10/site-packages/torch/nn/modules/container.py:250\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    248\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m    249\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[0;32m--> 250\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    251\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[0;32m~/miniconda3/envs/DL24/lib/python3.10/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/DL24/lib/python3.10/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/miniconda3/envs/DL24/lib/python3.10/site-packages/torch/nn/modules/conv.py:554\u001b[0m, in \u001b[0;36mConv2d.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    553\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 554\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_conv_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/DL24/lib/python3.10/site-packages/torch/nn/modules/conv.py:549\u001b[0m, in \u001b[0;36mConv2d._conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    537\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mzeros\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    538\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mconv2d(\n\u001b[1;32m    539\u001b[0m         F\u001b[38;5;241m.\u001b[39mpad(\n\u001b[1;32m    540\u001b[0m             \u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    547\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgroups,\n\u001b[1;32m    548\u001b[0m     )\n\u001b[0;32m--> 549\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv2d\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    550\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdilation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgroups\u001b[49m\n\u001b[1;32m    551\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Input type (torch.FloatTensor) and weight type (torch.cuda.FloatTensor) should be the same or input should be a MKLDNN tensor and weight is a dense tensor"
     ]
    }
   ],
   "source": [
    "encoder_output = encoder(audio_signal=mel_spectrograms, length=torch.tensor([mel_spectrograms.shape[2]] * 64))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65822003-a922-48d4-b784-76ea743351cd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30f4e367-5815-4c84-bfdf-272bdb1100d3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7d14190-6d7b-4a80-b659-791a44e3ec98",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6eefea3-e015-494b-93bd-a0c5bfe98fb6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "778542d5-e323-471a-98c3-426cefd6923e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "570308b0-16e0-4441-b8a6-23f2e781f2e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_signal = batch['anchor']\n",
    "signal_length = torch.tensor([input_signal.shape[-1]] * input_signal.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d949d562-7a8e-420e-9630-be9efe90a696",
   "metadata": {},
   "source": [
    "# nvidia tutrial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "908fa6fa-df4b-456b-aecd-7b7f003c5b31",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nemo.collections.asr.models import ASRModel, SLUIntentSlotBPEModel, SpeechEncDecSelfSupervisedModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "id": "6aef66f3-9410-4020-876b-16583d965b89",
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrained_encoder_name = 'stt_en_conformer_ctc_large'\n",
    "pretrained_encoder_name = 'stt_en_conformer_ctc_small'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "id": "e93f98d0-4cbd-4250-877b-eaf17ec4ef0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_cls = ASRModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "id": "4dd3a96a-a6a8-4fb5-a7fc-4645bf2ad092",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2024-10-31 17:32:00 cloud:58] Found existing object /home/evgenshuben/.cache/torch/NeMo/NeMo_1.23.0/stt_en_conformer_ctc_small/5d2d8e5b2b5adb8f5091363c6ba19c55/stt_en_conformer_ctc_small.nemo.\n",
      "[NeMo I 2024-10-31 17:32:00 cloud:64] Re-using file from: /home/evgenshuben/.cache/torch/NeMo/NeMo_1.23.0/stt_en_conformer_ctc_small/5d2d8e5b2b5adb8f5091363c6ba19c55/stt_en_conformer_ctc_small.nemo\n",
      "[NeMo I 2024-10-31 17:32:00 common:924] Instantiating model from pre-trained checkpoint\n",
      "[NeMo I 2024-10-31 17:32:01 mixins:172] Tokenizer SentencePieceTokenizer initialized with 1024 tokens\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NeMo W 2024-10-31 17:32:01 modelPT:165] If you intend to do training or fine-tuning, please call the ModelPT.setup_training_data() method and provide a valid configuration file to setup the train data loader.\n",
      "    Train config : \n",
      "    manifest_filepath: /data/NeMo_ASR_SET/English/v2.0/train/tarred_audio_manifest.json\n",
      "    sample_rate: 16000\n",
      "    batch_size: 64\n",
      "    shuffle: true\n",
      "    num_workers: 8\n",
      "    pin_memory: true\n",
      "    use_start_end_token: false\n",
      "    trim_silence: false\n",
      "    max_duration: 20.0\n",
      "    min_duration: 0.1\n",
      "    shuffle_n: 2048\n",
      "    is_tarred: true\n",
      "    tarred_audio_filepaths: /data/NeMo_ASR_SET/English/v2.0/train/audio__OP_0..4095_CL_.tar\n",
      "    \n",
      "[NeMo W 2024-10-31 17:32:01 modelPT:172] If you intend to do validation, please call the ModelPT.setup_validation_data() or ModelPT.setup_multiple_validation_data() method and provide a valid configuration file to setup the validation data loader(s). \n",
      "    Validation config : \n",
      "    manifest_filepath:\n",
      "    - /data/ASR/LibriSpeech/librispeech_withsp2/manifests/librivox-dev-other.json\n",
      "    - /data/ASR/LibriSpeech/librispeech_withsp2/manifests/librivox-dev-clean.json\n",
      "    - /data/ASR/LibriSpeech/librispeech_withsp2/manifests/librivox-test-other.json\n",
      "    - /data/ASR/LibriSpeech/librispeech_withsp2/manifests/librivox-test-clean.json\n",
      "    sample_rate: 16000\n",
      "    batch_size: 64\n",
      "    shuffle: false\n",
      "    num_workers: 8\n",
      "    pin_memory: true\n",
      "    use_start_end_token: false\n",
      "    is_tarred: false\n",
      "    tarred_audio_filepaths: na\n",
      "    \n",
      "[NeMo W 2024-10-31 17:32:01 modelPT:178] Please call the ModelPT.setup_test_data() or ModelPT.setup_multiple_test_data() method and provide a valid configuration file to setup the test data loader(s).\n",
      "    Test config : \n",
      "    manifest_filepath:\n",
      "    - /data/ASR/LibriSpeech/librispeech_withsp2/manifests/librivox-test-other.json\n",
      "    - /data/ASR/LibriSpeech/librispeech_withsp2/manifests/librivox-dev-clean.json\n",
      "    - /data/ASR/LibriSpeech/librispeech_withsp2/manifests/librivox-dev-other.json\n",
      "    - /data/ASR/LibriSpeech/librispeech_withsp2/manifests/librivox-test-clean.json\n",
      "    sample_rate: 16000\n",
      "    batch_size: 64\n",
      "    shuffle: false\n",
      "    num_workers: 8\n",
      "    pin_memory: true\n",
      "    use_start_end_token: false\n",
      "    is_tarred: false\n",
      "    tarred_audio_filepaths: na\n",
      "    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2024-10-31 17:32:01 features:289] PADDING: 0\n",
      "[NeMo I 2024-10-31 17:32:01 save_restore_connector:249] Model EncDecCTCModelBPE was successfully restored from /home/evgenshuben/.cache/torch/NeMo/NeMo_1.23.0/stt_en_conformer_ctc_small/5d2d8e5b2b5adb8f5091363c6ba19c55/stt_en_conformer_ctc_small.nemo.\n"
     ]
    }
   ],
   "source": [
    "pretraind_model = model_cls.from_pretrained(\n",
    "    model_name=pretrained_encoder_name, map_location=torch.device(\"cpu\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "id": "ecf629c1-b5e6-43df-a2ef-1385f261fb86",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = pretraind_model.encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "id": "4e346d23-a11d-4e77-9be8-466cc4af5dec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ConformerEncoder(\n",
       "  (pre_encode): ConvSubsampling(\n",
       "    (out): Linear(in_features=3520, out_features=176, bias=True)\n",
       "    (conv): Sequential(\n",
       "      (0): Conv2d(1, 176, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "      (1): ReLU(inplace=True)\n",
       "      (2): Conv2d(176, 176, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "      (3): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (pos_enc): RelPositionalEncoding(\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (layers): ModuleList(\n",
       "    (0-15): 16 x ConformerLayer(\n",
       "      (norm_feed_forward1): LayerNorm((176,), eps=1e-05, elementwise_affine=True)\n",
       "      (feed_forward1): ConformerFeedForward(\n",
       "        (linear1): Linear(in_features=176, out_features=704, bias=True)\n",
       "        (activation): Swish()\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (linear2): Linear(in_features=704, out_features=176, bias=True)\n",
       "      )\n",
       "      (norm_conv): LayerNorm((176,), eps=1e-05, elementwise_affine=True)\n",
       "      (conv): ConformerConvolution(\n",
       "        (pointwise_conv1): Conv1d(176, 352, kernel_size=(1,), stride=(1,))\n",
       "        (depthwise_conv): CausalConv1D(176, 176, kernel_size=(31,), stride=(1,), groups=176)\n",
       "        (batch_norm): BatchNorm1d(176, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (activation): Swish()\n",
       "        (pointwise_conv2): Conv1d(176, 176, kernel_size=(1,), stride=(1,))\n",
       "      )\n",
       "      (norm_self_att): LayerNorm((176,), eps=1e-05, elementwise_affine=True)\n",
       "      (self_attn): RelPositionMultiHeadAttention(\n",
       "        (linear_q): Linear(in_features=176, out_features=176, bias=True)\n",
       "        (linear_k): Linear(in_features=176, out_features=176, bias=True)\n",
       "        (linear_v): Linear(in_features=176, out_features=176, bias=True)\n",
       "        (linear_out): Linear(in_features=176, out_features=176, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (linear_pos): Linear(in_features=176, out_features=176, bias=False)\n",
       "      )\n",
       "      (norm_feed_forward2): LayerNorm((176,), eps=1e-05, elementwise_affine=True)\n",
       "      (feed_forward2): ConformerFeedForward(\n",
       "        (linear1): Linear(in_features=176, out_features=704, bias=True)\n",
       "        (activation): Swish()\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (linear2): Linear(in_features=704, out_features=176, bias=True)\n",
       "      )\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "      (norm_out): LayerNorm((176,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 188,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "id": "0d32229c-42b1-408a-99cd-c419263ef4d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 80, 50])"
      ]
     },
     "execution_count": 189,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch['anchor'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45619350-23d3-464c-a899-21fa55f74c82",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "id": "c1a0a569-4c05-4ba2-9005-3cbc5f21bd17",
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_signal = batch['anchor']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "id": "45e43fa4-bebd-4f0d-9bf6-0c37aef12e66",
   "metadata": {},
   "outputs": [],
   "source": [
    "mel_spectrograms = batch['anchor']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "id": "657a4e72-03ff-47ff-a938-5d10e13faf52",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50"
      ]
     },
     "execution_count": 192,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mel_spectrograms.shape[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "id": "660999f2-ab48-4a9d-bcef-fc855082ab69",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 80, 50])"
      ]
     },
     "execution_count": 193,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mel_spectrograms.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "id": "752889d6-78c9-4ffd-b865-6d37b06a6f5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "freq = 80  # Уменьшим размер частот для совместимости\n",
    "time = 50  # Увеличим временной размер для совместимости\n",
    "\n",
    "# Инициализация случайных данных для примера\n",
    "mel_spectrograms = torch.rand(batch_size, freq, time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "id": "08435cae-c867-47d9-a5e8-90ea8b4d4880",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 80, 50])"
      ]
     },
     "execution_count": 195,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mel_spectrograms.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "id": "4a4b920e-6a8d-4b34-b95a-f6c9ae7791c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_output = encoder(audio_signal=mel_spectrograms, length=torch.tensor([mel_spectrograms.shape[2]] * batch_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "id": "23ddb8e8-0c6b-4f39-bb8b-3462507ca419",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 176, 13])"
      ]
     },
     "execution_count": 198,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder_output[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8d6b517-b8e9-4408-a5ad-fbd1751cea63",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cd900c4-296b-4b61-ad76-fdfec33fc782",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3317488c-0937-4eab-91af-38285d920070",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fc17dfc-25f8-43fd-8d47-9e6555322524",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edef5aed-0170-47fa-8c71-f27cd5dbfc23",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder(audio_signal=processed_signal, length=processed_signal_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "5c80e5af-11bf-4ba8-b9a8-897620f219e1",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'int' object has no attribute 'to'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[58], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mencoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43maudio_signal\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprocessed_signal\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlength\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m50\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/DL24/lib/python3.10/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/DL24/lib/python3.10/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/miniconda3/envs/DL24/lib/python3.10/site-packages/nemo/core/classes/common.py:1098\u001b[0m, in \u001b[0;36mtypecheck.__call__\u001b[0;34m(self, wrapped, instance, args, kwargs)\u001b[0m\n\u001b[1;32m   1095\u001b[0m instance\u001b[38;5;241m.\u001b[39m_validate_input_types(input_types\u001b[38;5;241m=\u001b[39minput_types, ignore_collections\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mignore_collections, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1097\u001b[0m \u001b[38;5;66;03m# Call the method - this can be forward, or any other callable method\u001b[39;00m\n\u001b[0;32m-> 1098\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mwrapped\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1100\u001b[0m instance\u001b[38;5;241m.\u001b[39m_attach_and_validate_output_types(\n\u001b[1;32m   1101\u001b[0m     output_types\u001b[38;5;241m=\u001b[39moutput_types, ignore_collections\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mignore_collections, out_objects\u001b[38;5;241m=\u001b[39moutputs\n\u001b[1;32m   1102\u001b[0m )\n\u001b[1;32m   1104\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m outputs\n",
      "File \u001b[0;32m~/miniconda3/envs/DL24/lib/python3.10/site-packages/nemo/collections/asr/modules/conformer_encoder.py:491\u001b[0m, in \u001b[0;36mConformerEncoder.forward\u001b[0;34m(self, audio_signal, length, cache_last_channel, cache_last_time, cache_last_channel_len)\u001b[0m\n\u001b[1;32m    487\u001b[0m \u001b[38;5;129m@typecheck\u001b[39m()\n\u001b[1;32m    488\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\n\u001b[1;32m    489\u001b[0m     \u001b[38;5;28mself\u001b[39m, audio_signal, length, cache_last_channel\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, cache_last_time\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, cache_last_channel_len\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    490\u001b[0m ):\n\u001b[0;32m--> 491\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward_internal\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    492\u001b[0m \u001b[43m        \u001b[49m\u001b[43maudio_signal\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    493\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlength\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    494\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcache_last_channel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_last_channel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    495\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcache_last_time\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_last_time\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    496\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcache_last_channel_len\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_last_channel_len\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    497\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/DL24/lib/python3.10/site-packages/nemo/collections/asr/modules/conformer_encoder.py:521\u001b[0m, in \u001b[0;36mConformerEncoder.forward_internal\u001b[0;34m(self, audio_signal, length, cache_last_channel, cache_last_time, cache_last_channel_len)\u001b[0m\n\u001b[1;32m    519\u001b[0m     audio_signal \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpre_encode(audio_signal)\n\u001b[1;32m    520\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 521\u001b[0m     audio_signal, length \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpre_encode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maudio_signal\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlengths\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlength\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    522\u001b[0m     length \u001b[38;5;241m=\u001b[39m length\u001b[38;5;241m.\u001b[39mto(torch\u001b[38;5;241m.\u001b[39mint64)\n\u001b[1;32m    523\u001b[0m     \u001b[38;5;66;03m# self.streaming_cfg is set by setup_streaming_cfg(), called in the init\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/DL24/lib/python3.10/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/DL24/lib/python3.10/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/miniconda3/envs/DL24/lib/python3.10/site-packages/nemo/collections/asr/parts/submodules/subsampling.py:386\u001b[0m, in \u001b[0;36mConvSubsampling.forward\u001b[0;34m(self, x, lengths)\u001b[0m\n\u001b[1;32m    385\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x, lengths):\n\u001b[0;32m--> 386\u001b[0m     lengths \u001b[38;5;241m=\u001b[39m \u001b[43mcalc_length\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    387\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlengths\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    388\u001b[0m \u001b[43m        \u001b[49m\u001b[43mall_paddings\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_left_padding\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_right_padding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    389\u001b[0m \u001b[43m        \u001b[49m\u001b[43mkernel_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_kernel_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    390\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstride\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_stride\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    391\u001b[0m \u001b[43m        \u001b[49m\u001b[43mceil_mode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_ceil_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    392\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrepeat_num\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sampling_num\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    393\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    395\u001b[0m     \u001b[38;5;66;03m# Unsqueeze Channel Axis\u001b[39;00m\n\u001b[1;32m    396\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconv2d_subsampling:\n",
      "File \u001b[0;32m~/miniconda3/envs/DL24/lib/python3.10/site-packages/nemo/collections/asr/parts/submodules/subsampling.py:571\u001b[0m, in \u001b[0;36mcalc_length\u001b[0;34m(lengths, all_paddings, kernel_size, stride, ceil_mode, repeat_num)\u001b[0m\n\u001b[1;32m    569\u001b[0m one: \u001b[38;5;28mfloat\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1.0\u001b[39m\n\u001b[1;32m    570\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(repeat_num):\n\u001b[0;32m--> 571\u001b[0m     lengths \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mdiv(\u001b[43mlengths\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m(dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat) \u001b[38;5;241m+\u001b[39m add_pad, stride) \u001b[38;5;241m+\u001b[39m one\n\u001b[1;32m    572\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ceil_mode:\n\u001b[1;32m    573\u001b[0m         lengths \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mceil(lengths)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'int' object has no attribute 'to'"
     ]
    }
   ],
   "source": [
    "encoder(audio_signal=processed_signal, length=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "84de49a4-f68d-4b7b-a566-a1ae64f5a4d8",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "tuple index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[59], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m length\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mtensor([\u001b[43mprocessed_signal\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m]\u001b[49m] \u001b[38;5;241m*\u001b[39m processed_signal)\n",
      "\u001b[0;31mIndexError\u001b[0m: tuple index out of range"
     ]
    }
   ],
   "source": [
    "length=torch.tensor([processed_signal.shape[3]] * processed_signal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "382d62da-5099-4866-b4bd-93328f0919a6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 84, 50])"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processed_signal.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d60d66fa-9760-44e1-a6bc-36aa3b0c79de",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c0a3610-a1e7-4d29-8926-e576de5e762c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a52803c8-2836-4411-962f-810d7127e83c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26aef8f8-ded9-446e-b501-7719eb01c94e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bc3ba0f-f27b-41d4-9a5c-9c6bf8e41493",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23650a9d-5ee9-488b-97bf-d382cb1778b6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc8cc700-f82a-4046-977f-0b9a1f5eb052",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0586cea-52ee-4237-b3e3-56fc1765a6f8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62fd7086-91ef-49e3-b1ae-b32d4ddabfb4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10",
   "language": "python",
   "name": "dl24"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
