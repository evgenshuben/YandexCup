{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6eb5b8db-e4fd-4133-a9dc-5f231dda24e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "PROJECT_DIR = '/home/evgenshuben/Desktop/gitReps/YandexCup/'\n",
    "sys.path.append(PROJECT_DIR)\n",
    "os.chdir(PROJECT_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "52b72741-3722-445e-b774-c48f5e223968",
   "metadata": {},
   "outputs": [],
   "source": [
    "from omegaconf import OmegaConf\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4a25336c-b761-4a82-bc28-95f039690621",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from scr.train import Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c01e1e3e-4160-467a-8518-366718cbb66c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Путь к конфигурационному файлу, сохраненному Hydra\n",
    "config_path = \"/home/evgenshuben/Desktop/gitReps/YandexCup/outputs_val/run-0/hydra/.hydra/config.yaml\"\n",
    "\n",
    "\n",
    "cfg = OmegaConf.load(config_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9a92c2d0-5ab6-4e47-90cc-bf8f85f3deae",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "344d0820-4291-4354-ae69-50f3bcd10405",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ce3e1f98-2da1-474c-a24e-ffef822ed8e6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([84, 50])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train_dataloader.dataset[0]['anchor'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "2f9946e8-2092-48b0-a335-4ecc08a21f65",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class DepthwiseConv(nn.Module):\n",
    "    \"\"\"Глубинная свертка для Conformer.\"\"\"\n",
    "    def __init__(self, dim, kernel_size=10):  # Используем kernel_size=10\n",
    "        super(DepthwiseConv, self).__init__()\n",
    "        self.conv = nn.Conv1d(dim, dim, kernel_size, padding=kernel_size // 2, groups=dim)\n",
    "        self.pointwise = nn.Conv1d(dim, dim, kernel_size=1)\n",
    "        self.batch_norm = nn.BatchNorm1d(dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        x = F.gelu(x)\n",
    "        x = self.pointwise(x)\n",
    "        x = self.batch_norm(x)\n",
    "        return x\n",
    "\n",
    "class FeedForwardModule(nn.Module):\n",
    "    \"\"\"Feedforward модуль с промежуточной размерностью.\"\"\"\n",
    "    def __init__(self, dim, expansion_factor=4, dropout=0.1):\n",
    "        super(FeedForwardModule, self).__init__()\n",
    "        self.linear1 = nn.Linear(dim, dim * expansion_factor)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.linear2 = nn.Linear(dim * expansion_factor, dim)\n",
    "        self.layer_norm = nn.LayerNorm(dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "        x = self.layer_norm(x)\n",
    "        x = self.linear1(x)\n",
    "        x = F.gelu(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.linear2(x)\n",
    "        return x + residual\n",
    "\n",
    "class ConformerBlock(nn.Module):\n",
    "    \"\"\"Основной блок Conformer.\"\"\"\n",
    "    def __init__(self, dim, num_heads=8, expansion_factor=4, kernel_size=10, dropout=0.1):\n",
    "        super(ConformerBlock, self).__init__()\n",
    "        \n",
    "        # Проверка, что `dim` делится на `num_heads` для совместимости с MultiheadAttention\n",
    "        assert dim % num_heads == 0, \"dim должно делиться на num_heads без остатка\"\n",
    "        \n",
    "        self.ff1 = FeedForwardModule(dim, expansion_factor, dropout)\n",
    "        self.self_attn = nn.MultiheadAttention(dim, num_heads, dropout=dropout)\n",
    "        self.conv = DepthwiseConv(dim, kernel_size)\n",
    "        self.ff2 = FeedForwardModule(dim, expansion_factor, dropout)\n",
    "        self.layer_norm = nn.LayerNorm(dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Первичный feed-forward модуль\n",
    "        x = x + 0.5 * self.ff1(x)\n",
    "        \n",
    "        # Механизм внимания\n",
    "        residual = x\n",
    "        x = self.layer_norm(x)\n",
    "        x = x.transpose(0, 1)  # Перемещаем batch и временные размерности для MultiheadAttention\n",
    "        x, _ = self.self_attn(x, x, x)\n",
    "        x = self.dropout(x).transpose(0, 1) + residual  # Вернуть обратно размерности\n",
    "        \n",
    "        # Сверточный модуль\n",
    "        x = x + self.conv(x.transpose(1, 2)).transpose(1, 2)\n",
    "        \n",
    "        # Вторичный feed-forward модуль\n",
    "        x = x + 0.5 * self.ff2(x)\n",
    "        return x\n",
    "\n",
    "class Conformer(nn.Module):\n",
    "    def __init__(self, input_dim=84, num_blocks=4, dim=256, emb_size=128, num_heads=8, kernel_size=10, expansion_factor=4, dropout=0.1, num_classes=10):\n",
    "        super(Conformer, self).__init__()\n",
    "        \n",
    "        # Линейная проекция входа в требуемое количество измерений\n",
    "        self.input_projection = nn.Linear(input_dim, dim)\n",
    "        \n",
    "        # Стек conformer-блоков\n",
    "        self.conformer_blocks = nn.ModuleList([\n",
    "            ConformerBlock(dim, num_heads, expansion_factor, kernel_size, dropout)\n",
    "            for _ in range(num_blocks)\n",
    "        ])\n",
    "        \n",
    "        # Линейные головы для эмбеддингов и классификации\n",
    "        self.emb_head = nn.Linear(dim, emb_size)\n",
    "        self.cls_head = nn.Linear(dim, num_classes)\n",
    "        self.layer_norm = nn.LayerNorm(dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: (batch_size, 84, 50)\n",
    "        x = x.transpose(1, 2)  # Преобразовать в (batch_size, 50, 84)\n",
    "        x = self.input_projection(x)  # Преобразование размерности с 84 на dim\n",
    "        \n",
    "        # Прогон через conformer-блоки\n",
    "        for block in self.conformer_blocks:\n",
    "            x = block(x)\n",
    "        \n",
    "        x = self.layer_norm(x)\n",
    "        \n",
    "        # Усреднение по временной оси\n",
    "        pooled_x = x.mean(dim=1)  # (batch_size, dim)\n",
    "        \n",
    "        # Выходные эмбеддинги и классификационные предсказания\n",
    "        emb = self.emb_head(pooled_x)\n",
    "        cls = self.cls_head(pooled_x)\n",
    "        \n",
    "        return {\"emb\": emb, \"cls\": cls}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "248a13e3-53dc-4d70-81c1-41f1e3835d1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "for batch in trainer.train_dataloader:\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "418038cd-0e12-4adc-aa54-c540f6c4a175",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Conformer(input_dim=84, num_blocks=4, dim=256, emb_size=128, num_heads=8, kernel_size=10, expansion_factor=4, dropout=0.1, num_classes=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "82ccef1d-d91d-42e8-9c42-a0a28d374933",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([200, 84, 50])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch['anchor'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "99165acd-6877-4851-bf87-eea66208ee0f",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (50) must match the size of tensor b (51) at non-singleton dimension 1",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[40], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43manchor\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/DL24/lib/python3.10/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/DL24/lib/python3.10/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[0;32mIn[36], line 96\u001b[0m, in \u001b[0;36mConformer.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     94\u001b[0m \u001b[38;5;66;03m# Прогон через conformer-блоки\u001b[39;00m\n\u001b[1;32m     95\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m block \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconformer_blocks:\n\u001b[0;32m---> 96\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[43mblock\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     98\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayer_norm(x)\n\u001b[1;32m    100\u001b[0m \u001b[38;5;66;03m# Усреднение по временной оси\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/DL24/lib/python3.10/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/DL24/lib/python3.10/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[0;32mIn[36], line 65\u001b[0m, in \u001b[0;36mConformerBlock.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     62\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout(x)\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m+\u001b[39m residual  \u001b[38;5;66;03m# Вернуть обратно размерности\u001b[39;00m\n\u001b[1;32m     64\u001b[0m \u001b[38;5;66;03m# Сверточный модуль\u001b[39;00m\n\u001b[0;32m---> 65\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[43mx\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtranspose\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtranspose\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     67\u001b[0m \u001b[38;5;66;03m# Вторичный feed-forward модуль\u001b[39;00m\n\u001b[1;32m     68\u001b[0m x \u001b[38;5;241m=\u001b[39m x \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m0.5\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mff2(x)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: The size of tensor a (50) must match the size of tensor b (51) at non-singleton dimension 1"
     ]
    }
   ],
   "source": [
    "model(batch['anchor'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5de416cb-6454-45c0-8a73-ac7be7cfff18",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10",
   "language": "python",
   "name": "dl24"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
